{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA-seq Pipeline for Detecting Exitrons (Exonic Introns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm # progress tracker\n",
    "import pyranges as pr # parsing gff\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse Regtools Data \n",
    "- Parse raw junction data from regtools output files\n",
    "\n",
    "- Processes 1 file at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseJunctionFile(file_path):\n",
    "    # column names for RegTools junction files\n",
    "    regtools_column_names = [\n",
    "        'chrom', 'start_anchor', 'end_anchor', 'name', 'score', 'strand',\n",
    "        'thick_start_orig', 'thick_end_orig', 'item_rgb_orig',\n",
    "        'block_count_orig', 'block_sizes_orig', 'block_starts_orig'\n",
    "    ]\n",
    "    \n",
    "    # extract sample ID from the filename\n",
    "    sample_id = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    # read the file into a pandas DataFrame\n",
    "    df = pd.read_csv(\n",
    "        file_path, sep='\\t', header=None, names=regtools_column_names,\n",
    "        dtype={'chrom': str, 'block_sizes_orig': str, 'block_starts_orig': str}\n",
    "    )\n",
    "        \n",
    "    df['sample_id_source'] = sample_id\n",
    "\n",
    "    # convert relevant columns to numeric types, coercing errors\n",
    "    for col in ['start_anchor', 'end_anchor', 'score']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # drop rows if info is missing\n",
    "    df.dropna(subset=['start_anchor', 'end_anchor', 'score', 'block_sizes_orig'], inplace=True)\n",
    "    \n",
    "    # ensure int types\n",
    "    for col in ['start_anchor', 'end_anchor', 'score']:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform Junction Data ** optimization needed **\n",
    "- Recalculates junction coordinates, following Regtools documentation to take into account blockSize\n",
    "\n",
    "- Recalculates block size to represent length of junction\n",
    "\n",
    "- Outputs junction info in BED12 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # initialize lists for each column\\n    chrom_list = []\\n    chromStart_list = []\\n    chromEnd_list = []\\n    name_list = []\\n    score_list = []\\n    strand_list = []\\n    thickStart_list = []\\n    thickEnd_list = []\\n    itemRgb_list = []\\n    blockCount_list = []\\n    blockSizes_list = []\\n    blockStarts_list = []\\n    sample_id_source_list = []\\n\\n    # iterate over the filtered df\\n\\n    for i, (index, row) in enumerate(raw_df_filtered.iterrows()):\\n\\n        regtools_start = row[\\'start_anchor\\']\\n        regtools_end = row[\\'end_anchor\\']\\n        regtools_block_sizes_str = row[\\'block_sizes_orig\\']\\n\\n        parsed_block_sizes = [int(s) for s in regtools_block_sizes_str.strip(\\',\\').split(\\',\\')]\\n        if len(parsed_block_sizes) < 2:\\n            skipped_rows += 1\\n            continue\\n\\n        overhang_left = parsed_block_sizes[0]\\n        overhang_right = parsed_block_sizes[1]\\n\\n        junc_start = regtools_start + overhang_left\\n        junc_end = regtools_end - overhang_right\\n\\n        if junc_start >= junc_end: \\n            skipped_rows += 1\\n            continue\\n\\n        junc_length = junc_end - junc_start\\n\\n        # add values to respective lists\\n        chrom_list.append(row[\\'chrom\\'])\\n        chromStart_list.append(junc_start)\\n        chromEnd_list.append(junc_end)\\n        name_list.append(row[\\'name\\'])\\n        score_list.append(row[\\'score\\'])\\n        strand_list.append(row[\\'strand\\'])\\n        thickStart_list.append(junc_start)\\n        thickEnd_list.append(junc_end)\\n        itemRgb_list.append(row.get(\\'item_rgb_orig\\', \\'0\\'))\\n        blockCount_list.append(1)\\n        blockSizes_list.append(str(junc_length))\\n        blockStarts_list.append(\"0\")\\n        sample_id_source_list.append(row[\\'sample_id_source\\'])\\n\\n    # create df from dictionary of lists\\n    transformed_df = pd.DataFrame({\\n        \\'chrom\\': chrom_list,\\n        \\'chromStart\\': chromStart_list,\\n        \\'chromEnd\\': chromEnd_list,\\n        \\'name\\': name_list,\\n        \\'score\\': score_list,\\n        \\'strand\\': strand_list,\\n        \\'thickStart\\': thickStart_list,\\n        \\'thickEnd\\': thickEnd_list,\\n        \\'itemRgb\\': itemRgb_list,\\n        \\'blockCount\\': blockCount_list,\\n        \\'blockSizes\\': blockSizes_list,\\n        \\'blockStarts\\': blockStarts_list,\\n        \\'sample_id_source\\': sample_id_source_list\\n    })\\n\\n    print(f\"Transformed {len(transformed_df)} junction records.\")\\n    return transformed_df\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformJunctionData(raw_df):\n",
    "    \n",
    "    # CHROMOSOME FILTERING\n",
    "    original_row_count = len(raw_df)\n",
    "    \n",
    "    # allowed chromosomes\n",
    "    allowed_chrom_numbers = [str(i) for i in range(1, 23)]\n",
    "    allowed_sex_chroms_upper = ['X', 'Y'] \n",
    "    allowed_chromosomes = set()\n",
    "    for num_chrom in allowed_chrom_numbers:\n",
    "        allowed_chromosomes.add(num_chrom)\n",
    "        allowed_chromosomes.add(f\"chr{num_chrom}\")\n",
    "    for sex_chrom in allowed_sex_chroms_upper:\n",
    "        allowed_chromosomes.add(sex_chrom)\n",
    "        allowed_chromosomes.add(sex_chrom.lower())\n",
    "        allowed_chromosomes.add(f\"chr{sex_chrom}\")\n",
    "        allowed_chromosomes.add(f\"chr{sex_chrom.lower()}\")\n",
    "    \n",
    "    raw_df_filtered = raw_df[raw_df['chrom'].isin(allowed_chromosomes)].copy()\n",
    "    filtered_row_count = len(raw_df_filtered)\n",
    "    print(f\"Removed {original_row_count - filtered_row_count} rows with non-standard chromosomes.\")\n",
    "\n",
    "\n",
    "    # JUNCTION COORD CORRECTION\n",
    "    # filter rows for valid blocks\n",
    "    parsed_blocks_list = raw_df_filtered['block_sizes_orig'].str.strip(',').str.split(',')\n",
    "    has_sufficient_blocks = parsed_blocks_list.str.len() >= 2\n",
    "    raw_df_filtered = raw_df_filtered[has_sufficient_blocks].copy()\n",
    "    parsed_blocks_list = parsed_blocks_list[has_sufficient_blocks]\n",
    "    \n",
    "    # recalculating junction coordinates\n",
    "    raw_df_filtered.loc[:, 'overhang_left'] = parsed_blocks_list.str[0].astype(int)\n",
    "    raw_df_filtered.loc[:, 'overhang_right'] = parsed_blocks_list.str[1].astype(int)\n",
    "\n",
    "    junc_start = raw_df_filtered['start_anchor'] + raw_df_filtered['overhang_left']\n",
    "    junc_end = raw_df_filtered['end_anchor'] - raw_df_filtered['overhang_right']\n",
    "\n",
    "    # filter out invalid junctions\n",
    "    valid_junction = junc_start < junc_end\n",
    "    raw_df_filtered = raw_df_filtered[valid_junction].copy()\n",
    "    junc_start = junc_start[valid_junction]\n",
    "    junc_end = junc_end[valid_junction]\n",
    "\n",
    "\n",
    "    junc_length = junc_end - junc_start\n",
    "\n",
    "    # create df\n",
    "    transformed_df = pd.DataFrame()\n",
    "    transformed_df['chrom'] = raw_df_filtered['chrom']\n",
    "    transformed_df['chromStart'] = junc_start\n",
    "    transformed_df['chromEnd'] = junc_end\n",
    "    transformed_df['name'] = raw_df_filtered['name']\n",
    "    transformed_df['score'] = raw_df_filtered['score']\n",
    "    transformed_df['strand'] = raw_df_filtered['strand']\n",
    "    transformed_df['thickStart'] = junc_start\n",
    "    transformed_df['thickEnd'] = junc_end\n",
    "    transformed_df['itemRgb'] = raw_df_filtered['item_rgb_orig']\n",
    "    transformed_df['blockCount'] = 1\n",
    "    transformed_df['blockSizes'] = junc_length.astype(str)\n",
    "    transformed_df['blockStarts'] = \"0\"\n",
    "    transformed_df['sample_id_source'] = raw_df_filtered['sample_id_source']\n",
    "\n",
    "    print(f\"Transformed {len(transformed_df)} junction records.\")\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "'''\n",
    "    # initialize lists for each column\n",
    "    chrom_list = []\n",
    "    chromStart_list = []\n",
    "    chromEnd_list = []\n",
    "    name_list = []\n",
    "    score_list = []\n",
    "    strand_list = []\n",
    "    thickStart_list = []\n",
    "    thickEnd_list = []\n",
    "    itemRgb_list = []\n",
    "    blockCount_list = []\n",
    "    blockSizes_list = []\n",
    "    blockStarts_list = []\n",
    "    sample_id_source_list = []\n",
    "\n",
    "    # iterate over the filtered df\n",
    "\n",
    "    for i, (index, row) in enumerate(raw_df_filtered.iterrows()):\n",
    "\n",
    "        regtools_start = row['start_anchor']\n",
    "        regtools_end = row['end_anchor']\n",
    "        regtools_block_sizes_str = row['block_sizes_orig']\n",
    "\n",
    "        parsed_block_sizes = [int(s) for s in regtools_block_sizes_str.strip(',').split(',')]\n",
    "        if len(parsed_block_sizes) < 2:\n",
    "            skipped_rows += 1\n",
    "            continue\n",
    "        \n",
    "        overhang_left = parsed_block_sizes[0]\n",
    "        overhang_right = parsed_block_sizes[1]\n",
    "\n",
    "        junc_start = regtools_start + overhang_left\n",
    "        junc_end = regtools_end - overhang_right\n",
    "\n",
    "        if junc_start >= junc_end: \n",
    "            skipped_rows += 1\n",
    "            continue\n",
    "\n",
    "        junc_length = junc_end - junc_start\n",
    "\n",
    "        # add values to respective lists\n",
    "        chrom_list.append(row['chrom'])\n",
    "        chromStart_list.append(junc_start)\n",
    "        chromEnd_list.append(junc_end)\n",
    "        name_list.append(row['name'])\n",
    "        score_list.append(row['score'])\n",
    "        strand_list.append(row['strand'])\n",
    "        thickStart_list.append(junc_start)\n",
    "        thickEnd_list.append(junc_end)\n",
    "        itemRgb_list.append(row.get('item_rgb_orig', '0'))\n",
    "        blockCount_list.append(1)\n",
    "        blockSizes_list.append(str(junc_length))\n",
    "        blockStarts_list.append(\"0\")\n",
    "        sample_id_source_list.append(row['sample_id_source'])\n",
    "\n",
    "    # create df from dictionary of lists\n",
    "    transformed_df = pd.DataFrame({\n",
    "        'chrom': chrom_list,\n",
    "        'chromStart': chromStart_list,\n",
    "        'chromEnd': chromEnd_list,\n",
    "        'name': name_list,\n",
    "        'score': score_list,\n",
    "        'strand': strand_list,\n",
    "        'thickStart': thickStart_list,\n",
    "        'thickEnd': thickEnd_list,\n",
    "        'itemRgb': itemRgb_list,\n",
    "        'blockCount': blockCount_list,\n",
    "        'blockSizes': blockSizes_list,\n",
    "        'blockStarts': blockStarts_list,\n",
    "        'sample_id_source': sample_id_source_list\n",
    "    })\n",
    "    \n",
    "    print(f\"Transformed {len(transformed_df)} junction records.\")\n",
    "    return transformed_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find Exitrons Within Junction Data\n",
    "- Converts transformed junction data (transformed_df) and exon data (from gff3 file) into PyRanges objects with labels Chromosome, Start, End, Strand, and Title (a unique junction id formed by chrom:start:end:strand)\n",
    "\n",
    "- Finds junctions that overlap with CDS regions using PyRanges method .overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 903356 CDS regions.\n"
     ]
    }
   ],
   "source": [
    "# convert CDS data to PyRanges object\n",
    "gff = pr.read_gff3(\"gencode.v48.annotation.gff3.gz\")\n",
    "cds = gff[gff.Feature == \"CDS\"]\n",
    "print(f\"Found {len(cds)} CDS regions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findExitrons(transformed_df):\n",
    "    # generate a unique ID for each junction (chrom:start:end:strand\n",
    "    unique_id = transformed_df['chrom'].astype(str) + ':' + \\\n",
    "                transformed_df['chromStart'].astype(str) + ':' + \\\n",
    "                transformed_df['chromEnd'].astype(str) + ':' + \\\n",
    "                transformed_df['strand'].astype(str)\n",
    "\n",
    "    # convert junction data to PyRanges object\n",
    "    junction_pr = pr.PyRanges({'Chromosome': transformed_df['chrom'],\n",
    "                    'Start': transformed_df['chromStart'],\n",
    "                    'End': transformed_df['chromEnd'],\n",
    "                    'Strand': transformed_df['strand'],\n",
    "                    'title': unique_id,\n",
    "                    'reads': transformed_df['score'],\n",
    "                    'sourceID': transformed_df['sample_id_source']}) \n",
    "\n",
    "    # find overlapping junctions\n",
    "    contained_junctions = junction_pr.overlap(cds, contained_intervals_only=True)\n",
    "    print(f\"Found {len(contained_junctions)} junctions contained within CDS regions.\")\n",
    "            \n",
    "    return contained_junctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compile All Exitron Info\n",
    "- Iterates through each person's file, finding all exitron data then concatenating to a final matrix\n",
    "\n",
    "- Includes person ID (file name) and junction scores (total reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractExitronData(directory_path, file_pattern=\"*.bam.junc\"):\n",
    "\n",
    "    all_exitron_info = []\n",
    "    file_paths = glob.glob(os.path.join(directory_path, file_pattern))\n",
    "    print(f\"Found {len(file_paths)} files to process.\")\n",
    "\n",
    "    # testing first 5 out of 100\n",
    "    files_to_process = file_paths[:5]\n",
    "    print(f\"Processing the first {len(files_to_process)} files.\")\n",
    "\n",
    "    for file_path in tqdm(files_to_process):\n",
    "        print(\"Parsing new file...\")\n",
    "        file_name_only = os.path.basename(file_path)\n",
    "        try:\n",
    "            # 1.\n",
    "            parsed_data = parseJunctionFile(file_path)\n",
    "            # 2.\n",
    "            transformed_df = transformJunctionData(parsed_data)\n",
    "            # 3.\n",
    "            gr_file = findExitrons(transformed_df)\n",
    "            #4.\n",
    "            all_exitron_info.append(gr_file)\n",
    "\n",
    "        # skip to the next file if an error occurs\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file {file_name_only}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue \n",
    "\n",
    "    # concatenate all individual data into matrix \n",
    "    final_gr = pr.concat(all_exitron_info)\n",
    "    print(f\"\\nSuccessfully compiled exitron data from {len(all_exitron_info)} files.\")\n",
    "    return final_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files to process.\n",
      "Processing the first 5 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing new file...\n",
      "Removed 405 rows with non-standard chromosomes.\n",
      "Transformed 225089 junction records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_1803036/1365695747.py:18: UserWarning: overlap: 'auto' strand_behavior treated as ignore due to invalid Strand values. Please use strand_behavior=ignore\n",
      "  contained_junctions = junction_pr.overlap(cds, contained_intervals_only=True)\n",
      " 20%|██        | 1/5 [00:04<00:16,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5332 junctions contained within CDS regions.\n",
      "Parsing new file...\n",
      "Removed 686 rows with non-standard chromosomes.\n",
      "Transformed 277524 junction records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_1803036/1365695747.py:18: UserWarning: overlap: 'auto' strand_behavior treated as ignore due to invalid Strand values. Please use strand_behavior=ignore\n",
      "  contained_junctions = junction_pr.overlap(cds, contained_intervals_only=True)\n",
      " 40%|████      | 2/5 [00:09<00:13,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8193 junctions contained within CDS regions.\n",
      "Parsing new file...\n",
      "Removed 523 rows with non-standard chromosomes.\n",
      "Transformed 283082 junction records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_1803036/1365695747.py:18: UserWarning: overlap: 'auto' strand_behavior treated as ignore due to invalid Strand values. Please use strand_behavior=ignore\n",
      "  contained_junctions = junction_pr.overlap(cds, contained_intervals_only=True)\n",
      " 60%|██████    | 3/5 [00:14<00:09,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7228 junctions contained within CDS regions.\n",
      "Parsing new file...\n",
      "Removed 1009 rows with non-standard chromosomes.\n",
      "Transformed 301814 junction records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_1803036/1365695747.py:18: UserWarning: overlap: 'auto' strand_behavior treated as ignore due to invalid Strand values. Please use strand_behavior=ignore\n",
      "  contained_junctions = junction_pr.overlap(cds, contained_intervals_only=True)\n",
      " 80%|████████  | 4/5 [00:19<00:05,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9622 junctions contained within CDS regions.\n",
      "Parsing new file...\n",
      "Removed 671 rows with non-standard chromosomes.\n",
      "Transformed 270306 junction records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_1803036/1365695747.py:18: UserWarning: overlap: 'auto' strand_behavior treated as ignore due to invalid Strand values. Please use strand_behavior=ignore\n",
      "  contained_junctions = junction_pr.overlap(cds, contained_intervals_only=True)\n",
      "100%|██████████| 5/5 [00:24<00:00,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10160 junctions contained within CDS regions.\n",
      "\n",
      "Successfully compiled exitron data from 5 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chromosome</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Strand</th>\n",
       "      <th>title</th>\n",
       "      <th>reads</th>\n",
       "      <th>sourceID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>chr1</td>\n",
       "      <td>999787</td>\n",
       "      <td>999865</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1:999787:999865:-</td>\n",
       "      <td>2</td>\n",
       "      <td>CGND-HRA-00117-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1287596</td>\n",
       "      <td>1287672</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1:1287596:1287672:+</td>\n",
       "      <td>3</td>\n",
       "      <td>CGND-HRA-00117-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1295826</td>\n",
       "      <td>1295889</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1:1295826:1295889:-</td>\n",
       "      <td>1</td>\n",
       "      <td>CGND-HRA-00117-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1295826</td>\n",
       "      <td>1295889</td>\n",
       "      <td>-</td>\n",
       "      <td>chr1:1295826:1295889:-</td>\n",
       "      <td>1</td>\n",
       "      <td>CGND-HRA-00117-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>chr1</td>\n",
       "      <td>2029762</td>\n",
       "      <td>2029982</td>\n",
       "      <td>+</td>\n",
       "      <td>chr1:2029762:2029982:+</td>\n",
       "      <td>61</td>\n",
       "      <td>CGND-HRA-00117-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270114</th>\n",
       "      <td>chrY</td>\n",
       "      <td>13335800</td>\n",
       "      <td>13335891</td>\n",
       "      <td>-</td>\n",
       "      <td>chrY:13335800:13335891:-</td>\n",
       "      <td>1</td>\n",
       "      <td>CGND-HRA-00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270317</th>\n",
       "      <td>chrY</td>\n",
       "      <td>19709578</td>\n",
       "      <td>19709689</td>\n",
       "      <td>-</td>\n",
       "      <td>chrY:19709578:19709689:-</td>\n",
       "      <td>1</td>\n",
       "      <td>CGND-HRA-00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270317</th>\n",
       "      <td>chrY</td>\n",
       "      <td>19709578</td>\n",
       "      <td>19709689</td>\n",
       "      <td>-</td>\n",
       "      <td>chrY:19709578:19709689:-</td>\n",
       "      <td>1</td>\n",
       "      <td>CGND-HRA-00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270317</th>\n",
       "      <td>chrY</td>\n",
       "      <td>19709578</td>\n",
       "      <td>19709689</td>\n",
       "      <td>-</td>\n",
       "      <td>chrY:19709578:19709689:-</td>\n",
       "      <td>1</td>\n",
       "      <td>CGND-HRA-00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270317</th>\n",
       "      <td>chrY</td>\n",
       "      <td>19709578</td>\n",
       "      <td>19709689</td>\n",
       "      <td>-</td>\n",
       "      <td>chrY:19709578:19709689:-</td>\n",
       "      <td>1</td>\n",
       "      <td>CGND-HRA-00015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40535 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "index    |    Chromosome    Start     End       Strand    ...\n",
       "int64    |    object        int64     int64     object    ...\n",
       "-------  ---  ------------  --------  --------  --------  -----\n",
       "630      |    chr1          999787    999865    -         ...\n",
       "721      |    chr1          1287596   1287672   +         ...\n",
       "740      |    chr1          1295826   1295889   -         ...\n",
       "740      |    chr1          1295826   1295889   -         ...\n",
       "...      |    ...           ...       ...       ...       ...\n",
       "270317   |    chrY          19709578  19709689  -         ...\n",
       "270317   |    chrY          19709578  19709689  -         ...\n",
       "270317   |    chrY          19709578  19709689  -         ...\n",
       "270317   |    chrY          19709578  19709689  -         ...\n",
       "PyRanges with 40535 rows, 7 columns, and 1 index columns (with 28265 index duplicates). (3 columns not shown: \"title\", \"reads\", \"sourceID\").\n",
       "Contains 24 chromosomes and 3 strands (including non-genomic strands: ?)."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractExitronData(\"/gpfs/commons/groups/knowles_lab/atokolyi/als/juncs_min6bp/\",file_pattern=\"*.bam.junc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Summarize Exitron Info \n",
    "- Lists all unique exitrons and their counts\n",
    "\n",
    "- Identifies which exitrons are already annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Exitron Normalization\n",
    "- Divides exitron score by the reads of surrounding exons to find proportion of time that the exitron gets expressed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
